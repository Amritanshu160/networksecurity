{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8adfcfc",
   "metadata": {},
   "source": [
    "- .github/workflows folder so that further we will be focusing on deployment for which we will be using github-actions.\n",
    "- main.yml is basically for writing github actions so that we will proceed with the deployment and all.\n",
    "- Network_Data folder --> inside this i will upload my dataset.\n",
    "- Inside network security i m going to create my entire project structure.\n",
    "- Inside network security __init__.py file we will create bcoz i need to consider this entire folder --> networksecurity as a package.\n",
    "- This is why we use this __init__.py file.\n",
    "- Any constants that u will be defining will be in the constants folder.\n",
    "- The reason to create folder structure in networksecurity is to consider this like a package.\n",
    "- Pipeline folder for training and batch prediction pipeline.\n",
    "- utils folder -> any generic code that u specifically want to apply for the entire project u can create here in utils folder.\n",
    "- cloud --> this folder is for writing any information related to the cloud or functionalities realted to the cloud.\n",
    "- in all these folders we will go and try to create our .py files.\n",
    "- Dockerfile ---> to create docker image for all these files.\n",
    "- setup.py file ---> i will be writing some code which will be packaging this entire content itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3849dd",
   "metadata": {},
   "source": [
    "## Create github repository for this project then do the following commands\n",
    "- echo \"# networksecurity\" >> README.md\n",
    "- git init\n",
    "- git add README.md\n",
    "- git commit -m \"first commit\"\n",
    "- git branch -M main\n",
    "- git remote add origin https://github.com/Amritanshu160/networksecurity.git\n",
    "- git push -u origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c59f4f",
   "metadata": {},
   "source": [
    "- After git init ur notebooks folder will not be getting tracked initially bcoz its a empty folder.\n",
    "- git init\n",
    "- then : git add .  ---> it will take all the files currently here and will store it in the local repository.\n",
    "- then : git commit -m \"Project structure set up\"\n",
    "- Inside networksecurity not all folders will be added bcoz these are empty folders.\n",
    "- Then create main branch : git branch -M main\n",
    "- Then add remote repository : git remote add origin https://github.com/Amritanshu160/networksecurity.git  ---> repository link where we want to commit and push the code.\n",
    "- Then : git push -u origin main -----> origin is our (from where its going) , and main is (where it needs to go) i.e. our main branch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79370706",
   "metadata": {},
   "source": [
    "- As soon as u create files inside the folders present in networksecurity folder ur folders will start getting tracked --> U sign will be there ---> U means untracked here.\n",
    "- Inside each and every folder in networksecurity we will add __init__.py file --> Why ?? ---> Bcoz it will treat the entire folder(the folder in which __init__.py file is present) as a package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c277cf9",
   "metadata": {},
   "source": [
    "- After adding files in folders inside the networksecurity folder when u do : git add . ---> All the files will be in added mode.\n",
    "- then : git commit -m \"The Message\"\n",
    "- then : git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d37f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Now when u reload at github and go inside network security u will see the folders there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55968b41",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- If u do : git add . ----> all files checked\n",
    "- if u do : git add filename ----> then only that particular file will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3510c3d6",
   "metadata": {},
   "source": [
    "- First we will create logger.py\n",
    "- Then we created exception.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d42662",
   "metadata": {},
   "source": [
    "- Running exception.py file:\n",
    "- cd networksecurity\n",
    "- then cd exception\n",
    "- then: python exception.py\n",
    "- If the above fails , directly run from the root : python -m networksecurity.exception.exception\n",
    "- Run this in the exactly fresh terminal\n",
    "- Note: Parent folder me wapas jaane ke liye do : - cd.. then again cd.. , as we earlier did two time cd to go inside exception folder and to execute exception.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23192818",
   "metadata": {},
   "source": [
    "## Generic Project Structure\n",
    "- MongoDB Database ----> (Data Ingestion Config->Data Ingestion Component->Data Ingestion Artifacts) ---> (Data Validation Config->Data Validation Component->Data Validation Artifacts) ---> (Data Transformation Config->Data Transformation Component->Data Transformation Artifacts) --->\n",
    "(Model Trainer Config->Model Trainer Component->Model Trainer Artifacts) ---> (Model Evaluation Config->Model Evaluation Component->Model Evaluation Artifacts) ---> (Model Pusher Config->Model Pusher Component->Model Pusher Artifacts)   ------> Then Finally Push our model into the cloud(it can be Azure, AWS etc.)\n",
    "\n",
    "## How the Data will be coming in the MongoDB database ??\n",
    "- Here we need to understand something called as ETL Pipeline.\n",
    "- Extract,Transform and Load ---> 3 components make up the entire ETL pipeline\n",
    "- 3 important things in an ETL : 1.Source 2.Destination and between these two we have a step called as Transformation.\n",
    "- Here dataset is present in local -> Take the dataset --> Do basic preprocessing(Cleaning Raw Data,Transformation Process(Converting to some other format like json etc.))  ----> Then after preprocessing we will save it in some destination.\n",
    "- Here our source is our local from where we are reading our csv file ----> Then we will do preprocessing(where we will convert this into the json format) ---> Then we will save this in some destination database. In this scenario my destination database is nothing but its MongoDB.\n",
    "- This is how data comes in our MongoDB database.\n",
    "- Why is ETL important ??\n",
    "- Bcoz in real world problems this data will be coming from various sources and not just one source. In real world scenarios : My data will be coming from API's , S3 bukcet , Paid API's and some other multiple sources(like companies internal data).\n",
    "- So we combine all these data ----> Do transformation(Convert this into a json) ---> Then store it in Final Destination(Some Databse like MongoDb or AWS Dynamo DB, can be different databases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07487018",
   "metadata": {},
   "source": [
    "- NOTE: MongoDB will be in Atlas Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b8bb4",
   "metadata": {},
   "source": [
    "## MongoDB Atlas\n",
    "- Search for MongoDB atlas ---> create a free account.\n",
    "- Sign Up with Google.\n",
    "- There in ur profile page u will see clusters ---> A cluster named Cluster 0 will be there which will be paused , click on resume to resume ur cluster (this is a free cluster). It will then show that ur cluster is being created.\n",
    "- Once ur cluster will be created click on connect to connect with the cluster , then choose Drivers , in version choose python 3.6 or later. Note that pymongo needs to be installed. After all this click done.\n",
    "- After creating and connecting a cluster in the Choose a Connection Method page u will be able to see a option of : View Full Code Sample\n",
    "this is just to check whether my connection is working or not(pymongo library required -> helps u to connect with MongoDB itself).\n",
    "- In view full code sample ur uri will be there which will be ur MongoDB cluster connection.\n",
    "- Create a file to push data to MongoDB database : push_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83e224",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "- In data ingestion we read the data from our MongoDB database. We read this data --> it can be used for training our model.\n",
    "- We require here data ingestion config ---> it is nothing but these are soem basic informations like where my dataset needs to get stored, many steps that we can perform like we are doing feature engineering , convert data to train and test.\n",
    "- Data Ingestion Config contains info like : - Data Ingestion Dir - Feature Store File Path - Training File Path - Testing File Path - Train Test Split Ratio - Collection Name\n",
    "- Since we are exporting data from MongoDB it needs to be converted into a Raw CSV file. It should be stored like test.csv , train.csv.\n",
    "- After we get our data we will do some feature engineering.\n",
    "- Then split the data into train and test , Then give the data into Data Ingestion Artifact where then it will store the file as Train.csv and Test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440eca5d",
   "metadata": {},
   "source": [
    "## Data Validation Component\n",
    "- The artifact created in the data ingestion component is passed to the Data Validation component\n",
    "- We will create data validation config --> we will have all the necessary path informations\n",
    "- When we read data from MongoDB most important thing is that my data schema should not be changed.\n",
    "- Schema means feature names ,number of features should not be changed , also if a feature is following a normal distribution and then it starts following some other distrbution then that is called data drift----> this should not happen. If data drift happens then that same data cannot be used by our model for training bcoz then there will be huge difference.\n",
    "- Data Distribution changes with time  ---> Hence we should probably go and create data drift report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75546938",
   "metadata": {},
   "source": [
    "- Our dataset should have the same schema : i.e. same no. of columns, same no. of features, distribution should also be same.\n",
    "- 2nd thing we will be checking is the Data Drift  ---> Just to check whether the distribution of the data is same or not when we compare it with training the data (data we have used for training our model) and the new data that we specifically get.\n",
    "- 3rd : Validate no. of columns , Whether numerical columns exist or not and many more checks.\n",
    "- Basic Info Required in Data Validation Config : - Data Validation Dir - Valid Data Dir - Invalid Data Dir - Valid Train File Path - Invalid Train File Path - Invalid Test File Path - Drift Report File Path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81322679",
   "metadata": {},
   "source": [
    "- Next we will initate the Data Validation ---> From the Data Ingestion Artifact , from the ingested folder we are going to read the train, test csv. Data Ingestion Artifact is given as a input to Data Validation(Folder named ingested --- Which contains Train.csv and Test.csv)\n",
    "- Next step : Validate no. of columns ---> Train and Test data both should have same no. of columns.\n",
    "- The above will give us a status : Whether True or False  ---> Columns and missing or not.\n",
    "- Another check : Whether numerical column exist or not w.r.t our training data and test data  ---> Here also we will get a status.\n",
    "- Next step : Detect Data Drift ---> To check whether distribution of Data is changing or not.\n",
    "- How to check the above(Dataset drift) ?? ---> There is a mathematical way. ---> Here also we will get status , whether True or False.\n",
    "- Data Validation Component will return : Validation Status, Valid Train File Path, Valid Test File Path, Invalid Train File Path, Invalid Test File Path, Drift Report File Path.\n",
    "- All these will be in Data Validation Artifact Folder , along with report_yaml."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
